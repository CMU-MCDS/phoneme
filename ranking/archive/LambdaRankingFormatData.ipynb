{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.preprocessing import normalize\n",
    "root = \"/Users/yuyanzhang/Desktop/Capstone/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2365, 2365)\n"
     ]
    }
   ],
   "source": [
    "### Load baseline experiment result: Get ranking ###\n",
    "baseline = pd.read_csv(root+\"baseline.csv\")\n",
    "baseline = baseline.set_index(\"v extra v \\ trg >>\")\n",
    "lang_set = baseline.columns.values\n",
    "baseline.head()\n",
    "table = []\n",
    "\n",
    "header = [\"Target\",\"Training\",\"Baseline Score\"]\n",
    "# header.extend(category)\n",
    "table.append(header)\n",
    "rank = [\"Rank\"]\n",
    "for lan_target in lang_set:\n",
    "    score = []\n",
    "    for lan_train in lang_set: \n",
    "        if(lan_target == lan_train or baseline.loc[lan_train, lan_target] == 'X'):\n",
    "            continue\n",
    "        row = [lan_target, lan_train, baseline.loc[lan_train, lan_target]]\n",
    "        table.append(row)\n",
    "        score.append(baseline.loc[lan_train, lan_target])\n",
    "    \n",
    "    rank.extend(rankdata(score))\n",
    "        \n",
    "print(len(table), len(rank))   \n",
    "table = np.column_stack((np.array(table), np.array(rank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Target' 'Training' 'Baseline Score' 'Rank']\n",
      " ['ara' 'aze' '0.286' '24.0']\n",
      " ['ara' 'bel' '0.2854' '17.0']\n",
      " ...\n",
      " ['vie' 'tha' '0.2444' '14.5']\n",
      " ['vie' 'ukr' '0.2427' '5.0']\n",
      " ['vie' 'urd' '0.2457' '25.0']]\n"
     ]
    }
   ],
   "source": [
    "print(table)\n",
    "#print(pd.DataFrame(table).sort_values([0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_root = \"/Users/yuyanzhang/Desktop/Capstone/extracted/\"\n",
    "ttr_table = pd.read_csv(extracted_root+\"TTR.csv\")\n",
    "ttr_table = ttr_table.set_index(\"Lang\")\n",
    "overlap_word_table = pd.read_csv(extracted_root+\"Overlap_wordlevel.csv\")\n",
    "overlap_word_table = overlap_word_table.set_index(\"lang\")\n",
    "overlap_subword_table = pd.read_csv(extracted_root+\"Overlap_subwordlevel.csv\")\n",
    "overlap_subword_table = overlap_subword_table.set_index(\"lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2365, 2365)\n"
     ]
    }
   ],
   "source": [
    "### Reformat features: TTR & overlap ###\n",
    "extracted_type = [[\"TTR\", \"Overlap_wordlevel\",\"Overlap_subwordlevel\"]]\n",
    "for lan_target in lang_set:   \n",
    "    for lan_train in lang_set: \n",
    "        if(lan_target == lan_train or baseline.loc[lan_train, lan_target] == 'X'):\n",
    "            continue\n",
    "        ttr = ttr_table.loc[lan_train].values[0]\n",
    "        overlap_word = overlap_word_table.loc[lan_train, lan_target]\n",
    "        overlap_subword = overlap_subword_table.loc[lan_train, lan_target]\n",
    "        row = [ttr, overlap_word, overlap_subword]\n",
    "        extracted_type.append(row)\n",
    "        \n",
    "   \n",
    "        \n",
    "print(len(table), len(extracted_type))   \n",
    "table = np.column_stack((np.array(table), np.array(extracted_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOGRAPHIC\n",
      "GENETIC\n",
      "SYNTACTIC\n",
      "FEATURAL\n",
      "INVENTORY\n",
      "PHONOLOGICAL\n"
     ]
    }
   ],
   "source": [
    "### Reformat features: distances ###\n",
    "distance_root = \"/Users/yuyanzhang/Desktop/Capstone/uriel_v0_2/distances/\"\n",
    "distance_category = [a.split(\".\")[0] for a in os.listdir(distance_root) if a.split(\".\")[1]==\"csv\"]\n",
    "\n",
    "for item in distance_category:\n",
    "    print(item)\n",
    "    distance = [item]\n",
    "    geo_dist = pd.read_csv(distance_root+item+\".csv\")\n",
    "    geo_dist = geo_dist.set_index('G_CODE')\n",
    "    geo_dist_sub = geo_dist[geo_dist.columns.intersection(lang_set)]\n",
    "    geo_dist_sub = geo_dist_sub[geo_dist_sub.index.isin(lang_set)]\n",
    "    count = 1\n",
    "    for lan_target in lang_set:\n",
    "        for lan_train in lang_set:\n",
    "            if(lan_target == lan_train or baseline.loc[lan_train, lan_target] == 'X'):\n",
    "                continue\n",
    "            distance.append(geo_dist_sub.loc[lan_target, lan_train])\n",
    "            count += 1\n",
    "    table = np.column_stack((np.array(table), np.array(distance)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2365, 2853)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2843</th>\n",
       "      <th>2844</th>\n",
       "      <th>2845</th>\n",
       "      <th>2846</th>\n",
       "      <th>2847</th>\n",
       "      <th>2848</th>\n",
       "      <th>2849</th>\n",
       "      <th>2850</th>\n",
       "      <th>2851</th>\n",
       "      <th>2852</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Target</td>\n",
       "      <td>Training</td>\n",
       "      <td>Baseline Score</td>\n",
       "      <td>Rank</td>\n",
       "      <td>TTR</td>\n",
       "      <td>Overlap_wordlevel</td>\n",
       "      <td>Overlap_subwordlevel</td>\n",
       "      <td>GEOGRAPHIC</td>\n",
       "      <td>GENETIC</td>\n",
       "      <td>SYNTACTIC</td>\n",
       "      <td>...</td>\n",
       "      <td>S_COMPLEMENTIZER_WORD_AFTER_CLAUSE</td>\n",
       "      <td>S_OBLIQUE_AFTER_VERB</td>\n",
       "      <td>S_OBLIQUE_AFTER_OBJECT</td>\n",
       "      <td>S_OBLIQUE_BEFORE_VERB</td>\n",
       "      <td>S_OBLIQUE_BEFORE_OBJECT</td>\n",
       "      <td>S_ARTICLE_WORD_BEFORE_NOUN</td>\n",
       "      <td>S_ARTICLE_WORD_AFTER_NOUN</td>\n",
       "      <td>P_CODAS</td>\n",
       "      <td>P_COMPLEX_CODAS</td>\n",
       "      <td>P_LONG_VOWELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ara</td>\n",
       "      <td>aze</td>\n",
       "      <td>0.286</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.3311775201</td>\n",
       "      <td>292.0</td>\n",
       "      <td>153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ara</td>\n",
       "      <td>bel</td>\n",
       "      <td>0.2854</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.3341467083</td>\n",
       "      <td>222.0</td>\n",
       "      <td>145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7441</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ara</td>\n",
       "      <td>ben</td>\n",
       "      <td>0.2867</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.24244448600000001</td>\n",
       "      <td>42.0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ara</td>\n",
       "      <td>bos</td>\n",
       "      <td>0.2878</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.29039334149999996</td>\n",
       "      <td>306.0</td>\n",
       "      <td>148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7016</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2853 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1               2     3                    4     \\\n",
       "0  Target  Training  Baseline Score  Rank                  TTR   \n",
       "1     ara       aze           0.286  24.0         0.3311775201   \n",
       "2     ara       bel          0.2854  17.0         0.3341467083   \n",
       "3     ara       ben          0.2867  28.0  0.24244448600000001   \n",
       "4     ara       bos          0.2878  31.0  0.29039334149999996   \n",
       "\n",
       "                5                     6           7        8          9     \\\n",
       "0  Overlap_wordlevel  Overlap_subwordlevel  GEOGRAPHIC  GENETIC  SYNTACTIC   \n",
       "1              292.0                   153         1.0      1.0     0.0002   \n",
       "2              222.0                   145         1.0      1.0     0.7441   \n",
       "3               42.0                    97         1.0      1.0     0.6704   \n",
       "4              306.0                   148         1.0      1.0     0.7016   \n",
       "\n",
       "       ...                                      2843                  2844  \\\n",
       "0      ...        S_COMPLEMENTIZER_WORD_AFTER_CLAUSE  S_OBLIQUE_AFTER_VERB   \n",
       "1      ...                                        --                    --   \n",
       "2      ...                                        --                    --   \n",
       "3      ...                                        --                    --   \n",
       "4      ...                                        --                    --   \n",
       "\n",
       "                     2845                   2846                     2847  \\\n",
       "0  S_OBLIQUE_AFTER_OBJECT  S_OBLIQUE_BEFORE_VERB  S_OBLIQUE_BEFORE_OBJECT   \n",
       "1                      --                     --                       --   \n",
       "2                      --                     --                       --   \n",
       "3                      --                     --                       --   \n",
       "4                      --                     --                       --   \n",
       "\n",
       "                         2848                       2849     2850  \\\n",
       "0  S_ARTICLE_WORD_BEFORE_NOUN  S_ARTICLE_WORD_AFTER_NOUN  P_CODAS   \n",
       "1                          --                         --       --   \n",
       "2                          --                         --       --   \n",
       "3                          --                         --       --   \n",
       "4                          --                         --       --   \n",
       "\n",
       "              2851           2852  \n",
       "0  P_COMPLEX_CODAS  P_LONG_VOWELS  \n",
       "1               --             --  \n",
       "2               --             --  \n",
       "3               --             --  \n",
       "4               --             --  \n",
       "\n",
       "[5 rows x 2853 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('all_2.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "print(table.shape)\n",
    "\n",
    "pd.DataFrame(table).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2365, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuyanzhang/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "table = np.genfromtxt('all.csv',delimiter=',', dtype=None)\n",
    "print(table.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PHOIBLE_AA', 'SSWL', 'PHOIBLE_RA', 'PHOIBLE_PH', 'PHOIBLE_SPA', 'ETHNO', 'WALS', 'PHOIBLE_GM', 'PHOIBLE_UPSID', 'PHOIBLE_SAPHON']\n",
      "PHOIBLE_AA\n",
      "((2365, 13), (2365, 284))\n",
      "SSWL\n",
      "((2365, 297), (2365, 284))\n",
      "PHOIBLE_RA\n",
      "((2365, 581), (2365, 284))\n",
      "PHOIBLE_PH\n",
      "((2365, 865), (2365, 284))\n",
      "PHOIBLE_SPA\n",
      "((2365, 1149), (2365, 284))\n",
      "ETHNO\n",
      "((2365, 1433), (2365, 284))\n",
      "WALS\n",
      "((2365, 1717), (2365, 284))\n",
      "PHOIBLE_GM\n",
      "((2365, 2001), (2365, 284))\n",
      "PHOIBLE_UPSID\n",
      "((2365, 2285), (2365, 284))\n",
      "PHOIBLE_SAPHON\n",
      "((2365, 2569), (2365, 284))\n"
     ]
    }
   ],
   "source": [
    "### Reformat features: other ###\n",
    "other_root = \"/Users/yuyanzhang/Desktop/Capstone/uriel_v0_2/features/\"\n",
    "other_category = [a.split(\".\")[0] for a in os.listdir(other_root) if a.split(\".\")[1]==\"csv\" and a.split(\".\")[0] not in ['avg','predicted','all_sources']]\n",
    "print(other_category)\n",
    "\n",
    "\n",
    "for item in other_category:\n",
    "    print(item)\n",
    "    geo_dist = pd.read_csv(other_root+item+\".csv\")\n",
    "    geo_dist = geo_dist.set_index('G_CODE')\n",
    "    other = [list(geo_dist.columns.values)]\n",
    "    \n",
    "    count = 1\n",
    "    for lan_target in lang_set:\n",
    "        for lan_train in lang_set:\n",
    "            if(lan_target == lan_train or baseline.loc[lan_train, lan_target] == 'X'):\n",
    "                continue\n",
    "           \n",
    "            other.append(geo_dist.loc[lan_train].values)\n",
    "           \n",
    "            \n",
    "            count += 1\n",
    "    other = np.array(other).reshape(len(other), len(geo_dist.columns.values))\n",
    "    print(table.shape, np.array(other).shape)\n",
    "    table = np.column_stack((np.array(table), np.array(other)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2843</th>\n",
       "      <th>2844</th>\n",
       "      <th>2845</th>\n",
       "      <th>2846</th>\n",
       "      <th>2847</th>\n",
       "      <th>2848</th>\n",
       "      <th>2849</th>\n",
       "      <th>2850</th>\n",
       "      <th>2851</th>\n",
       "      <th>2852</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Target</td>\n",
       "      <td>Training</td>\n",
       "      <td>Baseline Score</td>\n",
       "      <td>Rank</td>\n",
       "      <td>TTR</td>\n",
       "      <td>Overlap_wordlevel</td>\n",
       "      <td>Overlap_subwordlevel</td>\n",
       "      <td>GEOGRAPHIC</td>\n",
       "      <td>GENETIC</td>\n",
       "      <td>SYNTACTIC</td>\n",
       "      <td>...</td>\n",
       "      <td>S_COMPLEMENTIZER_WORD_AFTER_CLAUSE</td>\n",
       "      <td>S_OBLIQUE_AFTER_VERB</td>\n",
       "      <td>S_OBLIQUE_AFTER_OBJECT</td>\n",
       "      <td>S_OBLIQUE_BEFORE_VERB</td>\n",
       "      <td>S_OBLIQUE_BEFORE_OBJECT</td>\n",
       "      <td>S_ARTICLE_WORD_BEFORE_NOUN</td>\n",
       "      <td>S_ARTICLE_WORD_AFTER_NOUN</td>\n",
       "      <td>P_CODAS</td>\n",
       "      <td>P_COMPLEX_CODAS</td>\n",
       "      <td>P_LONG_VOWELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ara</td>\n",
       "      <td>aze</td>\n",
       "      <td>0.286</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.3311775201</td>\n",
       "      <td>292.0</td>\n",
       "      <td>153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ara</td>\n",
       "      <td>bel</td>\n",
       "      <td>0.2854</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.3341467083</td>\n",
       "      <td>222.0</td>\n",
       "      <td>145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7441</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ara</td>\n",
       "      <td>ben</td>\n",
       "      <td>0.2867</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.24244448600000001</td>\n",
       "      <td>42.0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ara</td>\n",
       "      <td>bos</td>\n",
       "      <td>0.2878</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.29039334149999996</td>\n",
       "      <td>306.0</td>\n",
       "      <td>148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7016</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2853 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1               2     3                    4     \\\n",
       "0  Target  Training  Baseline Score  Rank                  TTR   \n",
       "1     ara       aze           0.286  24.0         0.3311775201   \n",
       "2     ara       bel          0.2854  17.0         0.3341467083   \n",
       "3     ara       ben          0.2867  28.0  0.24244448600000001   \n",
       "4     ara       bos          0.2878  31.0  0.29039334149999996   \n",
       "\n",
       "                5                     6           7        8          9     \\\n",
       "0  Overlap_wordlevel  Overlap_subwordlevel  GEOGRAPHIC  GENETIC  SYNTACTIC   \n",
       "1              292.0                   153         1.0      1.0     0.0002   \n",
       "2              222.0                   145         1.0      1.0     0.7441   \n",
       "3               42.0                    97         1.0      1.0     0.6704   \n",
       "4              306.0                   148         1.0      1.0     0.7016   \n",
       "\n",
       "       ...                                      2843                  2844  \\\n",
       "0      ...        S_COMPLEMENTIZER_WORD_AFTER_CLAUSE  S_OBLIQUE_AFTER_VERB   \n",
       "1      ...                                        --                    --   \n",
       "2      ...                                        --                    --   \n",
       "3      ...                                        --                    --   \n",
       "4      ...                                        --                    --   \n",
       "\n",
       "                     2845                   2846                     2847  \\\n",
       "0  S_OBLIQUE_AFTER_OBJECT  S_OBLIQUE_BEFORE_VERB  S_OBLIQUE_BEFORE_OBJECT   \n",
       "1                      --                     --                       --   \n",
       "2                      --                     --                       --   \n",
       "3                      --                     --                       --   \n",
       "4                      --                     --                       --   \n",
       "\n",
       "                         2848                       2849     2850  \\\n",
       "0  S_ARTICLE_WORD_BEFORE_NOUN  S_ARTICLE_WORD_AFTER_NOUN  P_CODAS   \n",
       "1                          --                         --       --   \n",
       "2                          --                         --       --   \n",
       "3                          --                         --       --   \n",
       "4                          --                         --       --   \n",
       "\n",
       "              2851           2852  \n",
       "0  P_COMPLEX_CODAS  P_LONG_VOWELS  \n",
       "1               --             --  \n",
       "2               --             --  \n",
       "3               --             --  \n",
       "4               --             --  \n",
       "\n",
       "[5 rows x 2853 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuyanzhang/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## Create file for ranking algorithm ##\n",
    "data = np.genfromtxt('all_2.csv', delimiter = \",\", dtype = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Group' 'Target' 'Training' 'Baseline Score' 'Rank' 'TTR'\n",
      " 'Overlap_wordlevel' 'Overlap_subwordlevel' 'GEOGRAPHIC' 'GENETIC'\n",
      " 'SYNTACTIC' 'FEATURAL' 'INVENTORY' 'PHONOLOGICAL']\n",
      "(2311, 14)\n",
      "(2311, 14)\n"
     ]
    }
   ],
   "source": [
    "data_sub = data[1:len(data_sub):, 0:14]\n",
    "print(data[0,0:14])\n",
    "print(data_sub.shape)\n",
    "\n",
    "#Normalize\n",
    "data_sub[:,5:len(data_sub[0])] = data_sub[:,5:len(data_sub[0])].astype(float)\n",
    "normalized = normalize(data_sub[:,5:len(data_sub[0])], axis=0)\n",
    "full = np.concatenate(( data_sub[:,[0,1,2,3,4]], normalized), axis=1)\n",
    "print(full.shape)\n",
    "np.random.shuffle(full)\n",
    "split = 0.7\n",
    "\n",
    "with open('rank.train','w') as f:\n",
    "    pass\n",
    "with open('rank.test','w') as f:\n",
    "    pass\n",
    "with open('rank.train.query','w') as f:\n",
    "    pass\n",
    "with open('rank.test.query','w') as f:\n",
    "    pass\n",
    "\n",
    "#Training\n",
    "idx = 0\n",
    "for row in full:    \n",
    "    group = row[0]\n",
    "    target = row[1]\n",
    "    training = row[2]\n",
    "    rank= str(int(float(row[4])))\n",
    "    feature = row[5:len(row)]\n",
    "    feature_dict = {k: v for k, v in enumerate(feature)}\n",
    "    out = [rank]\n",
    "    out.extend([str(k)+\":\"+str(v) for k,v in feature_dict.iteritems()])\n",
    "    if idx < len(data_sub)*split:\n",
    "        with open('rank.train','a') as f:\n",
    "            f.write(\" \".join(out)+\"\\n\")\n",
    "        with open('rank.train.query','a') as f:\n",
    "            f.write(str(1)+\"\\n\")\n",
    "    else:\n",
    "        with open('rank.test','a') as f:\n",
    "            f.write(\" \".join(out)+\"\\n\")\n",
    "        with open('rank.test.query','a') as f:\n",
    "            f.write(str(1)+\"\\n\")\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "label (31) excel the max range 1618",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-344-ec7c2905f55c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mq_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rank.test.query'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRanker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuyanzhang/anaconda2/lib/python2.7/site-packages/lightgbm/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, eval_at, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    808\u001b[0m                                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                                     \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                                     callbacks=callbacks)\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuyanzhang/anaconda2/lib/python2.7/site-packages/lightgbm/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    500\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuyanzhang/anaconda2/lib/python2.7/site-packages/lightgbm/engine.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuyanzhang/anaconda2/lib/python2.7/site-packages/lightgbm/basic.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1487\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m                 ctypes.byref(self.handle)))\n\u001b[0m\u001b[1;32m   1490\u001b[0m             \u001b[0;31m# save reference to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuyanzhang/anaconda2/lib/python2.7/site-packages/lightgbm/basic.pyc\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \"\"\"\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: label (31) excel the max range 1618"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import math\n",
    "import os\n",
    "import unittest\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn import __version__ as sk_version\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import (load_boston, load_breast_cancer, load_digits,\n",
    "                              load_iris, load_svmlight_file)\n",
    "from sklearn.exceptions import SkipTestWarning\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.utils.estimator_checks import (_yield_all_checks, SkipTest,check_parameters_default_constructible)\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train = load_svmlight_file('rank.train')\n",
    "X_test, y_test = load_svmlight_file('rank.test')\n",
    "q_train = np.loadtxt('rank.train.query')\n",
    "q_test = np.loadtxt('rank.test.query')\n",
    "gbm = lgb.LGBMRanker()\n",
    "gbm.fit(X_train, y_train, group=q_train, eval_set=[(X_test, y_test)],eval_group=[q_test], eval_at=[1, 3], early_stopping_rounds=5, verbose=False,callbacks=[lgb.reset_parameter(learning_rate=lambda x: 0.95 ** x * 0.1)])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
