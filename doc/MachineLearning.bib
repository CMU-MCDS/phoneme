% This file was created with JabRef 2.10.
% Encoding: MacRoman


@Article{Bahdanau2015,
  Title                    = {Neural Machine Translation by Jointly Learning to Align and Translate},
  Author                   = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  Journal                  = {ICLR 2015 (arXiv:1409.0473)},
  Year                     = {2015},

  Owner                    = {yuhsianglin},
  Timestamp                = {2018.01.12}
}

@Article{Bengio2009,
  Title                    = {Learning Deep Architectures for AI},
  Author                   = {Yoshua Bengio},
  Journal                  = {Foundations and Trends® in Machine Learning},
  Year                     = {2009},
  Number                   = {1},
  Pages                    = {1-127},
  Volume                   = {2},

  Doi                      = {10.1561/2200000006},
  ISSN                     = {1935-8237},
  Owner                    = {zachlin},
  Timestamp                = {2016.06.16},
  Url                      = {http://dx.doi.org/10.1561/2200000006}
}

@Article{Bengio2013,
  Title                    = {Representation Learning: A Review and New Perspectives},
  Author                   = {Y. Bengio and A. Courville and P. Vincent},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2013},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {1798-1828},
  Volume                   = {35},

  Doi                      = {10.1109/TPAMI.2013.50},
  ISSN                     = {0162-8828},
  Keywords                 = {artificial intelligence;data structures;probability;unsupervised learning;AI;autoencoders;data representation;density estimation;geometrical connections;machine learning algorithms;manifold learning;probabilistic models;representation learning;unsupervised feature learning;Abstracts;Feature extraction;Learning systems;Machine learning;Manifolds;Neural networks;Speech recognition;Boltzmann machine;Deep learning;autoencoder;feature learning;neural nets;representation learning;unsupervised learning;Algorithms;Artificial Intelligence;Humans;Neural Networks (Computer)},
  Owner                    = {zachlin},
  Timestamp                = {2016.06.16}
}

@Article{Bojanowski2016,
  Title                    = {Enriching Word Vectors with Subword Information},
  Author                   = {Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
  Journal                  = {arXiv:1607.04606},

  Owner                    = {yuhsianglin},
  Timestamp                = {2017.10.21}
}

@Article{Charalambous1992,
  Title                    = {Conjugate gradient algorithm for efficient training of artificial neural networks},
  Author                   = {C. Charalambous},
  Journal                  = {IEE Proceedings G - Circuits, Devices and Systems},
  Year                     = {1992},

  Month                    = {June},
  Number                   = {3},
  Pages                    = {301-310},
  Volume                   = {139},

  Doi                      = {10.1049/ip-g-2.1992.0050},
  ISSN                     = {0956-3768},
  Keywords                 = {conjugate gradient methods;learning systems;neural nets;artificial neural networks;conjugate gradient algorithm;line search algorithm;multilayer feedforward neural networks;Conjugate gradient methods;Learning systems;Neural networks},
  Owner                    = {zachlin},
  Timestamp                = {2016.08.21}
}

@Article{Chin2016,
  Title                    = {An Efficient Alternating Newton Method for Learning Factorization Machines},
  Author                   = {Chin, Wei-Sheng and Yuan, Bo-Wen and Yang, Meng-Yuan and Lin, Chih-Jen},
  Year                     = {2016},

  Owner                    = {zachlin},
  Timestamp                = {2016.08.28}
}

@Article{Deng2014,
  Title                    = {Deep Learning: Methods and Applications},
  Author                   = {Li Deng and Dong Yu},
  Journal                  = {Foundations and Trends® in Signal Processing},
  Year                     = {2014},
  Number                   = {3–4},
  Pages                    = {197-387},
  Volume                   = {7},

  Doi                      = {10.1561/2000000039},
  ISSN                     = {1932-8346},
  Owner                    = {zachlin},
  Timestamp                = {2016.06.16},
  Url                      = {http://dx.doi.org/10.1561/2000000039}
}

@Article{Elhoseiny2013,
  Title                    = {Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions},
  Author                   = {Mohamed Elhoseiny and Babak Saleh and Ahmed Elgammal},
  Journal                  = {International Conference on Computer Vision},
  Year                     = {2013},

  Owner                    = {yuhsianglin},
  Timestamp                = {2017.10.21}
}

@InProceedings{Goyal2018,
  Title                    = {A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models},
  Author                   = {Kartik Goyal and Graham Neubig and Chris Dyer and Taylor Berg-Kirkpatrick},
  Booktitle                = {Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)},
  Year                     = {2018},

  Address                  = {New Orleans, Louisiana},
  Month                    = {February},

  Url                      = {https://arxiv.org/abs/1708.00111}
}

@Article{Hagan1994,
  Title                    = {Training feedforward networks with the Marquardt algorithm},
  Author                   = {M. T. Hagan and M. B. Menhaj},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {1994},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {989-993},
  Volume                   = {5},

  Doi                      = {10.1109/72.329697},
  ISSN                     = {1045-9227},
  Keywords                 = {backpropagation;feedforward neural nets;function approximation;least squares approximations;Marquardt algorithm;backpropagation;feedforward network training;feedforward neural networks;function approximation;learning;nonlinear least squares;Acceleration;Approximation algorithms;Backpropagation algorithms;Convergence;Feedforward neural networks;Function approximation;Least squares approximation;Least squares methods;Neural networks;Testing},
  Owner                    = {zachlin},
  Timestamp                = {2016.08.21}
}

@Article{Johansson1991,
  Title                    = {BACKPROPAGATION LEARNING FOR MULTILAYER FEED-FORWARD NEURAL NETWORKS USING THE CONJUGATE GRADIENT METHOD},
  Author                   = {Johansson, E.M. and Dowla, F.U. and Goodman, D.M.},
  Journal                  = {International Journal of Neural Systems},
  Year                     = {1991},
  Number                   = {04},
  Pages                    = {291-301},
  Volume                   = {02},

  Doi                      = {10.1142/S0129065791000261},
  Eprint                   = {http://www.worldscientific.com/doi/pdf/10.1142/S0129065791000261},
  Owner                    = {zachlin},
  Timestamp                = {2016.08.21},
  Url                      = {http://www.worldscientific.com/doi/abs/10.1142/S0129065791000261}
}

@InProceedings{Kulis2011,
  Title                    = {What You Saw is Not What You Get: Domain Adaptation Using Asymmetric Kernel Transforms},
  Author                   = {Kulis, B. and Saenko, K. and Darrell, T.},
  Booktitle                = {Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2011},

  Address                  = {Washington, DC, USA},
  Pages                    = {1785--1792},
  Publisher                = {IEEE Computer Society},
  Series                   = {CVPR '11},

  __markedentry            = {[yuhsianglin:6]},
  Acmid                    = {2191798},
  Doi                      = {10.1109/CVPR.2011.5995702},
  ISBN                     = {978-1-4577-0394-2},
  Keywords                 = {imaging condition, asymmetric kernel transform, real-world application, visual domain adaptation, object model transfer, supervised learning, nonlinear transformation, kernel space, symmetric transformation, object recognition model},
  Numpages                 = {8},
  Owner                    = {yuhsianglin},
  Timestamp                = {2017.10.31},
  Url                      = {http://dx.doi.org/10.1109/CVPR.2011.5995702}
}

@Article{Li2015,
  Title                    = {Min-Max Kernels},
  Author                   = {Li, Ping},
  Journal                  = {arXiv:1503.01737},

  Owner                    = {zachlin},
  Timestamp                = {2016.08.19},
  Url                      = {http://arxiv.org/abs/1503.01737}
}

@Article{Li2016,
  Title                    = {Sign stable random projections for large-scale Learning},
  Author                   = {Li, Ping},
  Year                     = {2016},

  Owner                    = {zachlin},
  Timestamp                = {2016.08.28}
}

@Article{Lowe2017,
  Title                    = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  Author                   = {Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
  Journal                  = {arXiv:1706.02275},

  Owner                    = {yuhsianglin},
  Timestamp                = {2018.04.26},
  Url                      = {https://arxiv.org/abs/1706.02275}
}

@TechReport{Manasse2010,
  Title                    = {Consistent Weighted Sampling},
  Author                   = {Mark Manasse, Frank McSherry, Kunal Talwar},
  Year                     = {2010},
  Month                    = {June},

  Abstract                 = {

ABSTRACT
We describe an efficient procedure for sampling representatives from a weighted set such that for any weightings S and T, the probability that the two choose the same sample is equal to the Jaccard similarity between them: Pr[sample(S) = sample(T)] = sumx min(S(x), T(x)) / sumx max(S(x), T(x)) where sample(S) = (x, y) with 0 &lt; y&lt;= S(x). The sampling process takes expected time linear in the number of non-zero weights in S, independent of the weights themselves. We discuss and develop the implementation of our sampling schemes, reducing the requisite computation and randomness substantially. We additionally discuss a variety of settings in which weighted sampling with highly divergent weights is useful, for example TF-IDF style weighting for determining similarity of web pages, and privacy-preserving auctions.


},
  Owner                    = {zachlin},
  Publisher                = {Microsoft Research},
  Timestamp                = {2016.08.19},
  Url                      = {https://www.microsoft.com/en-us/research/publication/consistent-weighted-sampling/}
}

@Article{Mikolov2013,
  Title                    = {Efficient Estimation of Word Representations in Vector Space},
  Author                   = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  Journal                  = {arXiv:1301.3781},

  Owner                    = {yuhsianglin},
  Timestamp                = {2017.10.21}
}

@Article{Moeller1993,
  Title                    = {A scaled conjugate gradient algorithm for fast supervised learning },
  Author                   = {Martin Fodslette Moeller},
  Journal                  = {Neural Networks},
  Year                     = {1993},
  Number                   = {4},
  Pages                    = {525 - 533},
  Volume                   = {6},

  Doi                      = {http://dx.doi.org/10.1016/S0893-6080(05)80056-5},
  ISSN                     = {0893-6080},
  Keywords                 = {Feedforward neural network},
  Owner                    = {zachlin},
  Timestamp                = {2016.08.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0893608005800565}
}

@InProceedings{Nilsback2008,
  Title                    = {Automated flower classification over a large number of classes},
  Author                   = {M-E. Nilsback and A. Zisserman},
  Booktitle                = {Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing},
  Year                     = {2008},

  Owner                    = {yuhsianglin},
  Timestamp                = {2017.10.21}
}

@Article{Schmidhuber2015,
  Title                    = {Deep learning in neural networks: An overview},
  Author                   = {Jürgen Schmidhuber},
  Journal                  = {Neural Networks },
  Year                     = {2015},
  Pages                    = {85 - 117},
  Volume                   = {61},

  Doi                      = {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
  ISSN                     = {0893-6080},
  Keywords                 = {Deep learning},
  Owner                    = {zachlin},
  Timestamp                = {2016.06.16},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0893608014002135}
}

@Article{Schraudolph2002,
  Title                    = {Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent},
  Author                   = {Schraudolph, Nicol N.},
  Journal                  = {Neural Computation},
  Year                     = {2002},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {1723--1738},
  Volume                   = {14},

  Booktitle                = {Neural Computation},
  Comment                  = {doi: 10.1162/08997660260028683},
  Doi                      = {10.1162/08997660260028683},
  ISSN                     = {0899-7667},
  Owner                    = {zachlin},
  Publisher                = {MIT Press},
  Timestamp                = {2016.10.22},
  Url                      = {http://dx.doi.org/10.1162/08997660260028683}
}

@Article{Simonyan2014,
  Title                    = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  Author                   = {Simonyan, K. and Zisserman, A.},
  Journal                  = {arXiv:1409.1556}
}

@Article{Smagt1994,
  Title                    = {Minimisation methods for training feedforward neural networks },
  Author                   = {P. Patrick van der Smagt},
  Journal                  = {Neural Networks },
  Year                     = {1994},
  Number                   = {1},
  Pages                    = {1 - 11},
  Volume                   = {7},

  Doi                      = {http://dx.doi.org/10.1016/0893-6080(94)90052-3},
  ISSN                     = {0893-6080},
  Keywords                 = {Feedforward neural network training},
  Owner                    = {zachlin},
  Timestamp                = {2016.08.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0893608094900523}
}

@InProceedings{Towsey1995,
  Title                    = {Training a neural network with conjugate gradient methods},
  Author                   = {M. Towsey and D. Alpsan and L. Sztriha},
  Booktitle                = {Neural Networks, 1995. Proceedings., IEEE International Conference on},
  Year                     = {1995},
  Month                    = {Nov},
  Pages                    = {373-378 vol.1},
  Volume                   = {1},

  Doi                      = {10.1109/ICNN.1995.488128},
  Keywords                 = {conjugate gradient methods;convergence of numerical methods;learning (artificial intelligence);medical signal processing;multilayer perceptrons;optimisation;pattern classification;search problems;conjugate gradient methods;convergence;function estimation;function minimisation;global minimum;line search methods;medical signal classification problems;meta-optimisation;multilayer perceptron;neural network training;pattern classification;tuning parameters;Acceleration;Artificial neural networks;Backpropagation;Character generation;Convergence;Gradient methods;Neural networks;Optimization methods;Pattern classification;Search methods},
  Owner                    = {zachlin},
  Timestamp                = {2016.08.21}
}

@Article{Wang2016,
  Title                    = {The Common-directions Method for Regularized Empirical Risk Minimization},
  Author                   = {Wang, Po-Wei and Lee, Ching-pei and Lin, Chih-Jen},
  Year                     = {2016},

  Owner                    = {zachlin},
  Timestamp                = {2016.08.28}
}

@TechReport{Welinder2010,
  Title                    = {Caltech-UCSD Birds 200},
  Author                   = {P. Welinder and S. Branson and T. Mita and C. Wah and F. Schroff and S. Belongie and P. Perona},
  Institution              = {California Institute of Technology},
  Year                     = {2010},
  Number                   = {CNS-TR-2010-001}
}

@Article{You2016,
  Title                    = {Image Captioning with Semantic Attention},
  Author                   = {Quanzeng You and Hailin Jin and Zhaowen Wang and Chen Fang and Jiebo Luo},
  Journal                  = {arXiv:1603.03925},

  Owner                    = {yuhsianglin},
  Timestamp                = {2017.10.21}
}

@inproceedings{Neubig2018,
    title = {Rapid Adaptation of Neural Machine Translation to New Languages},
    author = {Graham Neubig and Junjie Hu},
    booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    address = {Brussels, Belgium},
    month = {November},
    url = {http://www.phontron.com/paper/neubig18emnlp.pdf},
    year = {2018}
}
